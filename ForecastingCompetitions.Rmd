---
title: Forecasting Competitions
author: Rob J Hyndman
date: 30 November 2018
titlefontsize: 32pt
colortheme: monashblue
toc: true
output:
  binb::monash:
    fig_height: 5
    fig_width: 8
    highlight: tango
    incremental: no
    keep_tex: yes
    toc: yes
    includes:
      in_header: preamble.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE, dev.args=list(bg=grey(0.9), pointsize=11))
library(tidyverse)
library(Mcomp)
library(fpp2)
library(patchwork)
```

# Who's the best forecaster?

## Who's the best forecaster?

\alert{Forecast each of the following things:}

  1. Google closing stock price on 12 December 2018.
  1. The seasonally adjusted estimate of total employment for Australia in November 2018.
  1. The total runs scored on Boxing Day 2018 at the MCG.

\begin{block}{}
For each of these, give a point forecast and an 80\% prediction interval.
\end{block}\pause

\begin{alertblock}{}
\begin{itemize}\tightlist
\item How will we measure best point forecast?
\item How will we measure best interval forecast?
\end{itemize}
\end{alertblock}

# Makridakis competitions

## Makridakis and Hibon (1979)

\full{MH79}
\only<2>{
\placefig{1}{4}{height=4cm,width=10cm,keepaspectratio}{SMakridakis}
\placefig{8.5}{4}{height=4cm,width=10cm,keepaspectratio}{MHibon}}

## Makridakis and Hibon (1979)

\begin{block}{}This was the first large-scale empirical evaluation of time series forecasting methods.
\end{block}

  * Highly controversial at the time.
  * Difficulties:

    - How to measure forecast accuracy?
    - How to apply methods consistently and objectively?
    - How to explain unexpected results?

  * Common thinking was that the more sophisticated mathematical models (ARIMA models at the time) were necessarily better.

## Makridakis and Hibon (1979)
\fontsize{13}{14}\sf

\begin{block}{}
I do not believe that it is very fruitful to attempt to classify series according to which forecasting techniques perform ``best''. The performance of any particular technique when applied to a particular series depends essentially on (a) the model which the series obeys; (b)~our ability to identify and fit this model correctly and (c)~the criterion chosen to measure the forecasting accuracy.\atright{\emph{--- M.B. Priestley}}
\end{block}

\begin{alertblock}<2->{}
\dots\ the paper suggests the application of normal scientific experimental design to forecasting, with measures of unbiased testing of forecasts against subsequent reality, for success or failure. A long overdue reform.\\ \atright{\hbox{\textit{--- F.H. Hansford-Miller}}}
\end{alertblock}

## Makridakis and Hibon (1979)
\fontsize{13}{14}\sf

###
Modern man is fascinated with the subject of forecasting \atright{\itshape --- W.G. Gilchrist}

\pause

###
It is amazing to me, however, that after all this exercise in identifying models, transforming and so on, that the autoregressive moving averages come out so badly. I wonder whether it might be partly due to the authors not using the backwards forecasting approach to obtain the initial errors.
\atright{\itshape --- W.G. Gilchrist}

## Makridakis and Hibon (1979)
\fontsize{13}{14}\sf

\begin{alertblock}{}
I find it hard to believe that Box-Jenkins, if properly applied, can actually be worse than so many of the simple methods\atright{\textit{--- C. Chatfield}}
\end{alertblock}
\begin{alertblock}<2->{}
Why do empirical studies sometimes give different answers? It may depend on the selected sample of time series, but I suspect it is more likely to depend on the skill of the analyst and on their individual interpretations of what is meant by Method $X$.\atright{\textit{--- C.~Chatfield}}
\end{alertblock}
\begin{alertblock}<3->{}
\dots\ these authors are more at home with simple procedures than with Box-Jenkins.\atright{\textit{--- C. Chatfield}}
\end{alertblock}

## Makridakis and Hibon (1979)
\fontsize{13}{14}\sf

\begin{block}{}
There is a fact that Professor Priestley must accept: empirical evidence is in \emph{disagreement} with his theoretical arguments.\atright{\textit{--- S. Makridakis \& M. Hibon}}
\end{block}\pause

\begin{block}{}
Dr Chatfield expresses some personal views about the first author \dots\ It might be useful for Dr Chatfield to read some of the psychological literature quoted in the main paper, and he can then learn a little more about biases and how they affect prior probabilities.\hfill\hbox{\itshape --- S. Makridakis \& M. Hibon}
\end{block}

## Consequences of M&H (1979)
\fontsize{14}{15}\sf
\begin{alertblock}{}
As a result of this paper, researchers started to:
\begin{itemize}\tightlist
\item[\textcolor{white}{\ding{229}}] consider how to automate forecasting methods;
\item[\textcolor{white}{\ding{229}}] study what methods give the best forecasts;
\item[\textcolor{white}{\ding{229}}] be aware of the dangers of over-fitting;
\item[\textcolor{white}{\ding{229}}] treat forecasting as a different problem from time series analysis.
\end{itemize}
\end{alertblock}
\pause

Makridakis \& Hibon followed up with a new competition in 1982:\vspace*{-0.2cm}

 * 1001 series from demography, industry, economics. Annual, quarterly, monthly.
 * Anyone could submit forecasts (avoiding the charge of incompetence)
 * Multiple forecast measures used.

## M-competition

\placefig{0.1}{1.4}{height=8.2cm,width=10cm}{M1}

\only<2>{\begin{textblock}{5.5}(6.5,2)
	\begin{block}{Best method: DSES}
	Classical multiplicative seasonal decomposition + Simple exponential smoothing applied to seasonally adjusted data, then reseasonalized.
	\end{block}\end{textblock}}
}

## M-competition

\alert{Main findings\hfill\small (taken from Makridakis \& Hibon, 2000)}

\begin{enumerate}
\item Statistically sophisticated or complex methods do not necessarily provide more accurate forecasts than simpler ones.
\item The relative ranking of the performance of the various methods varies according to the accuracy measure being used.
\item The accuracy when various methods are being combined outperforms, on average, the individual methods being combined and does very well in comparison to other methods.
\item The accuracy of the various methods depends upon the length of the forecasting horizon involved.
\end{enumerate}

## M3 competition

\full{M3paper}

## M3 competition
\fontsize{13}{14}\sf
\begin{block}{}
``The M3-Competition is a final attempt by the authors to settle the accuracy issue of
various time series methods\dots\ The extension involves the inclusion of more methods/ researchers (in particular in the areas of neural networks and expert systems) and more series.''
\end{block}

  * 3003 series
  * All data from business, demography, finance and economics.
  * Series length between 14 and 126.
  * Either non-seasonal, monthly or quarterly.
  * All time series positive.
  * M&H claimed that the M3-competition supported the findings of their earlier work.

## M3 results (recalculated)
\fontsize{13}{15}\sf
\begin{block}{}\centering
\begin{tabular}{lccc}
	\bf Method    & \bf MAPE & \bf sMAPE & \bf MASE \\ \midrule
	Theta         &  17.42   &   12.76   &   1.39   \\
	ForecastPro   &  18.00   &   13.06   &   1.47   \\
	ForecastX     &  17.35   &   13.09   &   1.42   \\
	Automatic ANN &  17.18   &   13.98   &   1.53   \\
	B-J automatic &  19.13   &   13.72   &   1.54
\end{tabular}\end{block}\vspace*{10cm}

\only<2->{\begin{textblock}{10.8}(1,6.2)
\begin{alertblock}{}
\begin{itemize}
\item[\textcolor{white}{\ding{228}}] Calculations do not match published paper.
\item[\textcolor{white}{\ding{228}}] Some contestants apparently submitted multiple entries but only best ones published.
\end{itemize}
\end{alertblock}
\end{textblock}}

## M3 competition
\fontsize{13}{14}\sf

### Theta

  * A very confusing explanation.
  * Shown by Hyndman and Billah (2003) to be average of linear regression and simple exponential smoothing with drift, applied to seasonally adjusted data.
  * Later, the original authors claimed that their explanation was incorrect.

### Forecast Pro

  * A commercial software package with an unknown algorithm.
  * Known to fit either exponential smoothing or ARIMA models using BIC.

## M4 competition

 * January -- May 2018
 * 100,000 time series: yearly, quarterly, monthly, weekly, daily, hourly.
 * Point forecast and prediction intervals assessed.
 * Code must be public
 * 248 registrations, 50 submissions.

\pause

### Winning methods
 1. Hybrid of Recurrent Neural Network and Exponential Smoothing models
 2. Forecast combination using xgboost to find weights

# Feature-based forecasting

## Forecast model selection

\alert{Features used to select a forecasting model}\vspace*{10cm}

\begin{textblock}{12}(0.1,2.1)\small
\begin{multicols}{2}
  \begin{itemize}\tightlist
    \item length
    \item strength of seasonality
    \item strength of trend
    \item linearity
    \item curvature
    \item spikiness
    \item stability
    \item lumpiness
    \item first ACF value of remainder series
    \item parameter estimates of Holt's linear trend method
    \item spectral entropy
    \item Hurst exponent
    \item nonlinearity
    \item parameter estimates of Holt-Winters' additive method
    \item unit root test statistics
    \item first ACF value of residual series of linear trend model
    \item ACF and PACF based features - calculated on both the raw and differenced series
    \end{itemize}
\end{multicols}
\end{textblock}

## \fontsize{16}{16}\bf\sffamily FFORMS: Feature-based FORecast Model Selection

\only<1>{\full{fw1}}
\only<2>{\full{fw2}}
\only<3>{\full{fw3}}
\only<4>{\full{fw4}}
\only<5>{\full{fw5}}
\only<6>{\full{fw6}}
\only<7>{\full{fw7}}
\only<8>{\full{fw8}}
\only<9>{\full{fw9}}
\only<10>{\full{fw10}}
\only<11>{\full{fw11}}
\only<12>{\full{fw12}}
\only<13>{\full{fw13}}
\only<14>{\full{fw14}}

\vspace*{10cm}

## Application to M competition data

\begin{block}{Experiment 1}
\centering\small\tabcolsep=0.1cm
\begin{tabular}{lrrrrr}
                 & Source & Y      & Q      & M \\
\midrule
Observed series  & M1     & 181    & 203    & 617 \\
Simulated series &        & 362000 & 406000 & 123400 \\
New series       & M3     & 645    & 756    & 1428
\end{tabular}
\end{block}
\begin{block}{Experiment 2}
\centering\small\tabcolsep=0.1cm
\begin{tabular}{lrrrrr}
                 & Source & Y       & Q       & M \\
\midrule
Observed series  & M3     & 645     & 756     & 1428 \\
Simulated series &        & 1290000 & 1512000 & 285600 \\
New series       & M1     & 181     & 203     & 617
\end{tabular}
\end{block}

## Results: Yearly

```{r, message=FALSE, warning=FALSE, echo=FALSE}
method <- c(
  "RF-unbalanced", "RF-class priors", "auto.arima", "ets", "WN", "RW", "RWD", "Theta",
  "RF-unbalanced", "RF-class priors", "auto.arima", "ets", "WN", "RW", "RWD", "Theta"
)
Rank <- c(
  1.50, 1.50, 3.33, 5.00, 8.00, 7.00, 3.67, 6.00,
  3.50, 2.50, 5.83, 4.67, 9.00, 8.00, 1.00, 3.50
)
class <- c(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1)
df <- data.frame(method = method, Rank = Rank, class = class)
ggplot(data = df, aes(x = method, y = Rank, fill = factor(class))) +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() +
  scale_x_discrete(
    limits = c("WN", "RW", "auto.arima", "ets", "Theta", "RWD", "RF-class priors", "RF-unbalanced"),
	    labels = c("WN", "RW", "auto.arima", "ets", "Theta", "RWD", "RF-class priors", "RF-unbalanced")
  ) + scale_fill_brewer(
    breaks = c(1, 0),
    labels = c("Experiment 1 (new: M3)", "Experiment 2 (new: M1)")
    , palette = "Set1"
  ) +
  theme(
    axis.title.y = element_blank(), legend.title = element_blank(),
    text = element_text(size = 20)
  )
```

## Results: Quarterly

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
method <- c(
  "RF-unbalanced", "RF-class priors", "auto.arima", "ets", "WN", "RW", "RWD", "STL-AR", "Theta", "Snaive",
  "RF-unbalanced", "RF-class priors", "auto.arima", "ets", "WN", "RW", "RWD", "STL-AR", "Theta", "Snaive"
)
Rank <- c(
  1.00, 2.63, 5.25, 3.00, 10.00, 7.50, 5.38, 8.63, 3.88, 7.75, 2.25,
  3.13, 4.75, 3.75, 10.00, 7.00, 6.50, 8.34, 2.50, 6.75
)
class <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
df <- data.frame(method = method, Rank = Rank, class = class)
ggplot(data = df, aes(x = method, y = Rank, fill = factor(class))) +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() +
  scale_x_discrete(
    limits = c("WN", "RW", "RWD", "STL-AR", "Snaive", "auto.arima", "ets", "Theta", "RF-class priors", "RF-unbalanced"),
    labels = c("WN", "RW", "RWD", "STL-AR", "Snaive", "auto.arima", "ets", "Theta", "RF-class priors", "RF-unbalanced")
  ) + scale_fill_brewer(
    breaks = c(1, 0),
    labels = c("Experiment 1 (new: M3)", "Experiment 2 (new: M1)")
    , palette = "Set1"
  ) +
  theme(
    axis.title.y = element_blank(), legend.title = element_blank(),
    text = element_text(size = 20)
  )
```

## Results: Monthly

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
method <- c(
  "RF-unbalanced", "RF-class priors", "auto.arima", "ets", "WN", "RW", "RWD", "STL-AR", "Theta", "Snaive",
  "RF-unbalanced", "RF-class priors", "auto.arima", "ets", "WN", "RW", "RWD", "STL-AR", "Theta", "Snaive"
)
Rank <- c(1.77, 2.83, 4.94, 3.44, 10.00, 7.25, 8.61, 7.38, 2.27, 6.47, 3.22, 2.00, 2.83, 2.72, 10.00, 8.03, 6.89, 7.89, 4.22, 7.19)
class <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
df <- data.frame(method = method, Rank = Rank, class = class)
ggplot(data = df, aes(x = method, y = Rank, fill = factor(class))) +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() +
  scale_x_discrete(
    limits = c("WN", "RW", "RWD", "STL-AR", "Snaive", "auto.arima", "ets", "Theta", "RF-class priors", "RF-unbalanced"),
    labels = c("WN", "RW", "RWD", "STL-AR", "Snaive", "auto.arima", "ets", "Theta", "RF-class priors", "RF-unbalanced")
  ) + scale_fill_brewer(
    breaks = c(1, 0),
    labels = c("Experiment 1 (new: M3)", "Experiment 2 (new: M1)")
    , palette = "Set1"
  ) +
  theme(
    axis.title.y = element_blank(), legend.title = element_blank(),
    text = element_text(size = 20)
  )
```

## \fontsize{15}{15}\sffamily\bfseries FFORMA: Feature-based FORecast Model Averaging

 * Like FFORMS but we use gradient boosted trees rather than a random forest.
 * The optimization criterion is forecast accuracy not classification accuracy.
 * The probability of each model being best is used to construct a model weight.
 * A combination forecast is produced using these weights.
 * \alert{Came second in the M4 forecasting competition}

## \fontsize{15}{15}\sffamily\bfseries FFORMA: Feature-based FORecast Model Averaging

### Models included

1. Naive
1. Seasonal naive
1. Random walk with drift
1. Theta method
1. ARIMA
1. ETS
1. TBATS
1. STLM-AR

## Are we getting better at forecasting?

```{r m1accuracy, include=FALSE, dependson='forecasts'}
load("m1.rda")

# Overall accuracy from M1
m1 %>%
  filter(
    Method %in% c("DSES", "Theta", "ARIMA", "ETS", "ETSARIMA", "FFORMA")
  ) %>%
  group_by(Method) %>%
  summarise(
    MAPE = mean(pe),
    sMAPE = mean(spe),
    MASE = mean(se),
  ) %>%
  mutate(
    original = case_when(
      Method == "DSES" ~ TRUE,
      TRUE ~ FALSE
    )
  ) ->
bestm1
```

```{r m3accuracy, include=FALSE, dependson='forecasts'}
load("m3.rda")

# Overall accuracy from M3
m3 %>%
  filter(
    Method %in% c("DSES", "Theta", "ARIMA", "ETS", "ETSARIMA", "FFORMA")
  ) %>%
  group_by(Method) %>%
  summarise(
    MAPE = mean(pe),
    sMAPE = mean(spe),
    MASE = mean(se),
  ) %>%
  mutate(
    original = case_when(
      Method == "DSES" ~ TRUE,
      TRUE ~ FALSE
    )
  ) ->
bestm3
```

\fontsize{12}{13}\sf

```{r tables, dependson=c("m1accuracy","m3accuracy")}
bind_cols(bestm1, bestm3) %>%
  select(-original, -original1, -Method1) %>%
  arrange(MASE) %>%
  knitr::kable(
    format = "latex", digits = c(0, 1, 1, 2, 1, 1, 2), booktabs = TRUE,
    col.names = c(
      "Method", "MAPE", "sMAPE", "MASE",
      "MAPE", "sMAPE", "MASE"
    )
  ) %>%
  kableExtra::kable_styling(latex_options = "hold_position") %>%
  kableExtra::add_header_above(c("", "M1 competition" = 3, "M3 competition" = 3))
```

## Are we getting better at forecasting?

  * DSES did well as measured by MAPE and sMAPE on the M1 data, but very poorly everywhere else.
  * While Theta did quite well on the M3 data, it performed poorly on the M1 data.
  * FFORMA outperforms the other methods on every measure for the M1 competition, and on all but MAPE for the M3 competition.
  * ETSARIMA is almost as good as FFORMA, and is much easier and faster to compute.

## Are we getting better at forecasting?

```{r graphs, dependson=c("m1accuracy","m3accuracy")}
# Accuracy by period
m1 %>%
  filter(
    Method %in% c("DSES", "Theta", "ARIMA", "ETS", "ETSARIMA", "FFORMA")
  ) %>%
  group_by(Period, Method) %>%
  summarise(
    MAPE = mean(pe),
    sMAPE = mean(spe),
    MASE = mean(se),
  ) %>%
  arrange(Period, MASE) %>%
  mutate(
    Method = factor(Method, levels = arrange(bestm1, -MASE)[["Method"]]),
  ) ->
bestperiod

p1mase <- bestperiod %>%
  ggplot(aes(y = Method, group = Period, x = MASE)) +
  geom_point() + facet_grid(Period ~ .) +
  ggtitle("M1 competition") +
  xlim(0.7, 5)

p1mape <- bestperiod %>%
  ggplot(aes(y = Method, group = Period, x = MAPE)) +
  geom_point() + facet_grid(Period ~ .) +
  ggtitle("M1 competition") +
  xlim(11, 22.5)

# Accuracy by period
m3 %>%
  filter(
    Method %in% c("DSES", "Theta", "ARIMA", "ETS", "ETSARIMA", "FFORMA"),
    Period != "OTHER"
  ) %>%
  group_by(Period, Method) %>%
  summarise(
    MAPE = mean(pe),
    sMAPE = mean(spe),
    MASE = mean(se),
  ) %>%
  arrange(Period, MASE) %>%
    mutate(
    Method = factor(Method, levels = arrange(bestm1, -MASE)[["Method"]]),
  ) ->
bestperiod

p3mase <- bestperiod %>%
  ggplot(aes(y = Method, group = Period, x = MASE)) +
  geom_point() + facet_grid(Period ~ .) +
  ggtitle("M3 competition") +
  xlim(0.7, 5)

p3mape <- bestperiod %>%
  ggplot(aes(y = Method, group = Period, x = MAPE)) +
  geom_point() + facet_grid(Period ~ .) +
  ggtitle("M3 competition") +
  xlim(11, 22.5)

p1mase | p1mape
```

## Are we getting better at forecasting?

```{r m3comp, dependson='graphs'}
p3mase | p3mape
```

# Kaggle and other competition platforms

## Kaggle

\full{kaggle}

## Kaggle and Melbourne

 * Started in Melbourne in 2010 by Anthony Goldbloom.
 * 2010 tourism forecasting competition won by Jeremy Howard (also from Melbourne)
 * Jeremy was Kaggle Chief Scientist, 2011--2013
 * Chess rankings competition won by Alec Stephenson (CSIRO, Melbourne) in 2012
 * Heritage health competition $500,000 prize winners included Phil Brierley (Melbourne consultant) in 2013.
 * Kaggle was purchased by Google in 2017

## Kaggle rip-offs

 * ChallengerAI
 * CrowdAI
 * CrowdAnalytix
 * DataFountain
 * DrivenData

# Forecasting case studies

## CASE STUDY 1: Paperware company

### Methods currently used

A
: 12 month average

C
: 6 month average

E
: straight line regression over last 12 months

G
: straight line regression over last 6 months

H
: average slope between last year's and this year's values.
  (Equivalent to differencing at lag 12 and taking mean.)

I
: Same as H except over 6 months.

K
: I couldn't understand the explanation.

## CASE STUDY 2: PBS

\full{pills}

## CASE STUDY 2: PBS

### The Pharmaceutical Benefits Scheme (PBS) is the Australian government drugs subsidy scheme.

  * Many drugs bought from pharmacies are subsidised to allow more equitable access to modern drugs.
  * The cost to government is determined by the number and types of drugs purchased. Currently nearly 1\% of GDP.
  * The total cost is budgeted based on forecasts of drug usage.

## CASE STUDY 2: PBS

\full{pbs2}

## CASE STUDY 2: PBS

  * In 2001: $4.5 billion budget, under-forecasted by $800 million.
  * Thousands of products. Seasonal demand.
  * Subject to covert marketing, volatile products, uncontrollable expenditure.
  * Although monthly data available for 10 years, data are aggregated to annual values, and only the first three years are used in estimating the forecasts.
  * All forecasts being done with the *FORECAST* function in MS-Excel!

## CASE STUDY 3: Car fleet company

**Client:** One of Australia's largest car fleet companies

**Problem:** how to forecast resale value of vehicles? How
should this affect leasing and sales policies?

\pause

### Additional information
 - They can provide a large amount of data on previous vehicles and their eventual resale values.
 - The resale values are currently estimated by a group of specialists. They see me as a threat and do not cooperate.

## CASE STUDY 4: Airline

\full{ansettlogo}

## CASE STUDY 4: Airline

```{r, echo=FALSE, fig.height=5}
autoplot(melsyd[,"Economy.Class"],
  main="Economy class passengers: Melbourne-Sydney",
  xlab="Year",ylab="Thousands")
```

## CASE STUDY 4: Airline

```{r, echo=FALSE, fig.height=5}
autoplot(melsyd[,"Economy.Class"],
  main="Economy class passengers: Melbourne-Sydney",
  xlab="Year",ylab="Thousands")
```

\begin{textblock}{4.2}(7,6.3)
\begin{alertblock}{}
Not the real data! Or is it?
\end{alertblock}
\end{textblock}

## CASE STUDY 4: Airline

**Problem:** how to forecast passenger traffic on major routes?

### Additional information

  * They can provide a large amount of data on previous routes.
  * Traffic is affected by school holidays, special events such as
the Grand Prix, advertising campaigns, competition behaviour, etc.
  * They have a highly capable team of people who are able to do
most of the computing.

